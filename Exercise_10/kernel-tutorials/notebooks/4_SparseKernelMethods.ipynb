{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we demonstrate how to adapt the kernel methods shown in the [previous notebook](3_KernelMethods.ipynb) to use sparse kernels.\n",
    "\n",
    "As for the previous notebooks, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. At the end of the notebook, we employ a \"skcosmo class\" for the model, which is found in the skcosmo module and contains all necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append('../')\n",
    "from utilities.general import load_variables, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection, plot_regression, check_mirrors, get_cmaps, table_from_dict\n",
    ")\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from skcosmo.decomposition import KernelPCovR\n",
    "from skcosmo.sample_selection import FPS\n",
    "from skcosmo.preprocessing import SparseKernelCenterer\n",
    "from functools import partial\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use('../utilities/kernel_pcovr.mplstyle')\n",
    "dbl_fig=(2*plt.rcParams['figure.figsize'][0], plt.rcParams['figure.figsize'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Sparse Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change this cell to change the kernel function throughout\n",
    "kernel_params = {\"kernel\": \"rbf\", \"gamma\": 1.0}\n",
    "kernel_func = partial(rbf_kernel, gamma=1.0)\n",
    "kernel_type = \"gaussian\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Nystr&ouml;m Approximation\n",
    "\n",
    "In sparse kernel methods, an approximate kernel is used in place of the full kernel. This approximate kernel is typically constructed according to the [Nystr&ouml;m approximation](https://en.wikipedia.org/wiki/Low-rank_matrix_approximations#Nystr%C3%B6m_approximation) [(Williams 2001)](http://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf),\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} \\approx \\mathbf{\\hat{K}}_{NN} = \\mathbf{K}_{NM} \\mathbf{K}_{MM}^{-1} \\mathbf{K}_{NM}^T\n",
    "\\end{equation}\n",
    "\n",
    "Here, $M$ represents a subset of the $N$ total rows/columns of the kernel matrix, i.e. the kernel between a small **active set** that is selected with subsampling method, like farthest point sampling (FPS) [(Eldar 1997)](https://doi.org/10.1109/83.623193), or a CUR decomposition [(Imbalzano2018)](https://doi.org/10.1063/1.5024611), that is discussed in the [next notebook](5_CUR.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In our imported data from `load_variables()`, `X_train` and `X_test` are pre-centered and pre-scaled relative to the train set. Additionally, the imported `K_train` and `K_test` kernels have been constructed using uncentered and unscaled $\\mathbf{X}$ data. If we want to compare the sparse kernels that we will soon construct to the imported \"full\" kernels, we also need to build the sparse kernels on uncentered and unscaled $\\mathbf{X}$ data. Therefore, we undo the scaling and centering on the imported $\\mathbf{X}$ data here, and re-center and re-scale the data after building the sparse kernels. In general, centering and scaling the $\\mathbf{X}$ data before building kernels is optional; however, one should be consistent when working with multiple kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train * X_scale + X_center\n",
    "X_test = X_test * X_scale + X_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_active = 20\n",
    "\n",
    "fps_selector = FPS(n_to_select=n_active)\n",
    "fps_idxs = fps_selector.fit(X_train).selected_idx_\n",
    "\n",
    "Xsparse = X_train[fps_idxs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $\\mathbf{K}_{NM}$ is the kernel matrix between input data $\\mathbf{X}$ and $\\mathbf{X_{sparse}}$, a version of $\\mathbf{X}$ containing only the active set. $\\mathbf{K}_{MM}$ is the matrix containing the kernel evaluated between the active set samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmm = kernel_func(Xsparse, Xsparse)\n",
    "Knm_train = kernel_func(X_train, Xsparse)\n",
    "Knm_test = kernel_func(X_test, Xsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Explicit RKHS\n",
    "\n",
    "Sometimes, it might be more convenient to explicitly write out the projection of the training points\n",
    "on the [RKHS](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space) defined by the active set.\n",
    "This is essentially a KPCA built for the active set, that is not truncated to a few eigenvectors,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\Phi}_{NM} = \\mathbf{K}_{NM} \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}.\n",
    "\\end{equation}\n",
    "\n",
    "Using this definition it is easy to derive the Nyström approximation: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{K}}_{NN} = \\mathbf{\\Phi}_{NM} \\mathbf{\\Phi}_{NM}^T = \n",
    "\\mathbf{K}_{NM}  \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1}  \\mathbf{U}_{MM}^T \\mathbf{K}_{NM}^T\n",
    "= \\mathbf{K}_{NM} \\mathbf{K}_{MM}^{-1} \\mathbf{K}_{NM}^T.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering the RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the \"full\" kernels in [the previous notebook](3_KernelMethods.ipynb) were centered, sparse kernels must also be centered relative to the train set. The goal is to ensure that the Nyström-approximated kernel $\\mathbf{\\hat{K}}_{NN}$ is centered relative to the training set. This is achieved by centering the approximate RKHS features $\\mathbf{\\Phi}_{NM}$, and we denote the centered version of the RKHS features as $\\mathbf{\\tilde{\\Phi}}_{NM} = \\mathbf{\\Phi}_{NM} - \\mathbf{\\bar{\\Phi}}_{M}$. If we represent each element of $\\mathbf{\\Phi}$ in its summation form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\Phi}_{nm} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(K_{nm'} U_{m'm}\\right), \n",
    "\\end{equation}\n",
    "\n",
    "then the column means are given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\bar{\\Phi}}_{m} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(\\left(\\frac{1}{N}\\sum_n^N K_{nm'}\\right)U_{m'm} \\right), \n",
    "\\end{equation}\n",
    "\n",
    "so the centered feature matrix is computed by $\\mathbf{K}_{NM}$, centered by the column means of the kernel, as denoted by $\\mathbf{\\bar{K}}_M$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{\\Phi}}_{NM} =  \\left(\\mathbf{K}_{NM} -\\mathbf{\\bar{K}}_M\\right) \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}.\n",
    "\\end{equation}\n",
    "\n",
    "It is best to keep the column mean $\\mathbf{\\bar{K}}_M$ separate, because it has to be used also when performing out-of-sample embedding, where $\\mathbf{K}_{NM}$ would corresponds to the test set kernel. For consistency, $\\mathbf{\\bar{K}}_M$ must always be the kernel mean associated with the **train set**.\n",
    "\n",
    "Alternatively, one can store $\\mathbf{\\bar{\\Phi}}_{M}$ and use it for centering.\n",
    "\n",
    "**Note**: in the following we often use $\\mathbf{\\Phi}_{NM}$ and $\\mathbf{\\tilde{\\Phi}}_{NM}$ without the subscripts to indicate the train set features approximated in the active RKHS.\n",
    "\n",
    "<!---\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{\\Phi}}_{nm} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(\\left(K_{nm'} - \\frac{1}{N}\\sum_{n'}^N K_{n'm'}\\right)U_{m'm}\\right)\n",
    "\\end{equation}\n",
    "\\end{comment}\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel between the active points $\\mathbf{K}_{MM}$ can also be centered independently, though this is optional and can lead to near-zero eigenvalues, as noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_center = np.mean(Knm_train, axis=0)\n",
    "\n",
    "Knm_train -= K_center\n",
    "Knm_test -= K_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to centering the kernel, one may also want to scale the sparse kernel(s) so that, for example, the trace of the Nyström-approximated train kernel is equal to the number of training points. To achieve this, the sparse kernel $\\mathbf{K}_{NM}$ is divided by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{\\frac{\\operatorname{Tr}(\\mathbf{K}_{NM}\\mathbf{K}_{MM}^{-1}\\mathbf{K}_{NM}^T)}{n_{train}}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{K}_{NM}$ refers to the kernel between the **train set** and the active points. The same scaling should be applied to both the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_scale = Knm_train @ np.linalg.pinv(Kmm, rcond=1.0E-12) @ Knm_train.T\n",
    "K_scale = np.sqrt(np.trace(K_scale) / Knm_train.shape[0])\n",
    "\n",
    "Knm_train /= K_scale\n",
    "Knm_test /= K_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing the RKHS features, it might be wise to discard some of the smaller eigenvalues. For instance, if it has been centered, $\\mathbf{K}_{MM}$ has one _exactly_ zero eigenvalue, and we should take it out of the projection. [(Honeine 2014)](https://arxiv.org/pdf/1407.2904.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmm, Umm = np.linalg.eigh(Kmm)\n",
    "\n",
    "# Umm/vmm are already sorted, but in *increasing* order, so reverse them\n",
    "Umm = np.flip(Umm, axis=1)\n",
    "vmm = np.flip(vmm, axis=0)\n",
    "\n",
    "Umm = Umm[:, vmm > 0]\n",
    "vmm = vmm[vmm > 0]\n",
    "\n",
    "Phi = (\n",
    "    Knm_train\n",
    "    @ Umm[:, : n_active - 1]\n",
    "    @ np.diagflat(1.0 / np.sqrt(vmm[0 : n_active - 1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-center and scale the X data\n",
    "X_train = (X_train - X_center) / X_scale\n",
    "X_test = (X_test - X_center) / X_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse kernel principal component analysis (sKPCA) is formulated in the same way as standard KPCA, with the exception that an approximate kernel matrix is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\tilde{\\Phi}}$ is the feature matrix for the train points in the RKHS defined by the $M$ active set. Sparse KPCA can be understood (and derived) as PCA in the active set RKHS, by computing and diagonalising the covariance matrix built from $\\mathbf{\\tilde{\\Phi}}$. The covariance should be computed using *centered* kernel features, as discussed above\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} = \\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}} = \\mathbf{U}_C \\mathbf{\\Lambda}_C \\mathbf{U}_C^T.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(Phi.T, Phi)\n",
    "\n",
    "v_C, U_C = np.linalg.eigh(C)\n",
    "\n",
    "# U_C/v_C are already sorted, but in *increasing* order, so reverse them\n",
    "U_C = np.flip(U_C, axis=1)\n",
    "v_C = np.flip(v_C, axis=0)\n",
    "\n",
    "U_C = U_C[:, v_C > 0]\n",
    "v_C = v_C[v_C > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting the Sparse KPCA\n",
    "Projecting into latent space, we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{T} &= \\mathbf{\\tilde{\\Phi}} \\hat{\\mathbf{U}}_C \\\\\n",
    "    &= \\left(\\mathbf{K}_{NM}- \\bar{\\mathbf{K}}_M\\right)\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\hat{\\mathbf{U}}_C \\\\\n",
    "    &= \\mathbf{K}_{NM}\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\hat{\\mathbf{U}}_C -\\bar{\\mathbf{\\Phi}}\\hat{\\mathbf{U}}_C\\\\\n",
    "    &= \\mathbf{K}_{NM} \\mathbf{P}_{KT} - \\mathbf{\\bar{T}}\n",
    "\\end{align}\n",
    "\n",
    "where our sKPCA projector from kernel space $\\mathbf{P}_{KT} = \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{\\hat{U}}_C$, where $\\mathbf{\\hat{U}}_C$ contains the first $n_{PCA}$ eigenvectors of $\\mathbf{C}$. $\\mathbf{\\bar{T}} = \\bar{\\mathbf{\\Phi}}\\hat{\\mathbf{U}}_C$ centers in the latent space, and is computed and stored for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = (\n",
    "    Umm[:, : n_active - 1]\n",
    "    @ np.diagflat(1.0 / np.sqrt(vmm[0 : n_active - 1]))\n",
    "    @ U_C[:, :n_PC]\n",
    ")\n",
    "\n",
    "T_train = Knm_train @ PKT\n",
    "T_test = Knm_test @ PKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref_kpca = KernelPCA(\n",
    "    n_components=n_PC, kernel=\"rbf\", gamma=1.0, fit_inverse_transform=True\n",
    ")\n",
    "ref_kpca.fit(X_train)\n",
    "T_kpca = ref_kpca.transform(X_test)\n",
    "\n",
    "plot_projection(\n",
    "    Y_test,\n",
    "    check_mirrors(T_test, T_kpca),\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=\"Sparse KPCA on {} Environments\".format(Kmm.shape[0]),\n",
    "    **cmaps\n",
    ")\n",
    "plot_projection(\n",
    "    Y_test,\n",
    "    T_kpca,\n",
    "    fig=fig,\n",
    "    ax=axes[1],\n",
    "    title=\"KPCA on {} Environments\".format(X_train.shape[0]),\n",
    "    **cmaps\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reconstruct $\\mathbf{X}$ using $\\mathbf{P}_{TX} = \\mathbf{\\Lambda}^{-1}\\mathbf{T}^T\\mathbf{X}$, as in KPCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.diagflat(1.0 / (v_C[:n_PC])) @ T_train.T @ X_train\n",
    "\n",
    "Xr_test = T_test @ PTX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "The same loss functions are used as in KPCA, so we can compare the loss with that of KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_approx_train = T_train @ T_train.T\n",
    "\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_approx_test = T_test @ T_test.T\n",
    "\n",
    "table_from_dict(\n",
    "    [\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            xr=ref_kpca.inverse_transform(T_kpca),\n",
    "            y=Y_test,\n",
    "            t=T_kpca,\n",
    "            k=K_test,\n",
    "            kapprox=T_kpca @ T_kpca.T,\n",
    "        ),\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            xr=Xr_test,\n",
    "            y=Y_test,\n",
    "            t=T_test,\n",
    "            k=K_test_test,\n",
    "            kapprox=K_approx_test,\n",
    "        ),\n",
    "    ],\n",
    "    headers=[\"KPCA\", \"sKPCA\"],\n",
    "    title=\"sKPCA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KRR\n",
    "\n",
    "## Sparse KRR Weights\n",
    "Let's see how sparsity works out in the case of regression. \n",
    "\n",
    "If we now build a (regularized) linear regression in the RKHS we get the loss\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\lVert \\mathbf{Y} - \\mathbf{\\tilde{\\Phi}}\\mathbf{P}_{\\mathbf{\\tilde{\\Phi}} Y} \\rVert^2 + \n",
    "\\lambda \\lVert\\mathbf{P}_{\\mathbf{\\tilde{\\Phi}} Y} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "This is solved by \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\mathbf{\\tilde{\\Phi}} Y} = \\left(\\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}}+ \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{\\tilde{\\Phi}}^T \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "or, by writing the last $\\mathbf{\\tilde{\\Phi}}^T$ in terms of the kernel:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\mathbf{\\tilde{\\Phi}} Y} = \\left(\\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}}+ \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{\\Lambda}_{MM}^{-1/2} \\mathbf{U}_{MM}^T \\mathbf{K}_{NM}^T  \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: For (kernel) ridge regression, we can use either the centered feature matrix $\\mathbf{\\tilde{\\Phi}}$ or the uncentered feature matrix $\\mathbf{\\Phi}$. If we use $\\mathbf{\\tilde{\\Phi}}$, the property matrix $\\mathbf{Y}$ must also be centered for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from after we've computed our sparse kernels and recompute $\\mathbf{\\tilde{\\Phi}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vmm, Umm = np.linalg.eigh(Kmm)\n",
    "\n",
    "# Umm/vmm are already sorted, but in *increasing* order, so reverse them\n",
    "Umm = np.flip(Umm, axis=1)\n",
    "vmm = np.flip(vmm, axis=0)\n",
    "\n",
    "Umm = Umm[:, vmm > 0]\n",
    "vmm = vmm[vmm > 0]\n",
    "\n",
    "Phi = (\n",
    "    Knm_train\n",
    "    @ Umm[:, : n_active - 1]\n",
    "    @ np.diagflat(1.0 / np.sqrt(vmm[0 : n_active - 1]))\n",
    ")\n",
    "\n",
    "PPY = Phi.T @ Phi\n",
    "PPY = PPY + regularization * np.eye(Phi.shape[1])\n",
    "PPY = np.linalg.pinv(PPY) @ Phi.T @ Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Often Cheaper, More Elegant Route\n",
    "\n",
    "We cast this expression into the more commonly used form by a series of simple manipulations that remove the need for diagonalizing $K_{MM}$ and computing $\\mathbf{\\tilde{\\Phi}}$. First, we redefine the weights so that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{\\Phi}}\\mathbf{P}_{\\mathbf{\\tilde{\\Phi}} Y} = \n",
    "\\mathbf{K}_{NM} \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\mathbf{P}_{\\mathbf{\\tilde{\\Phi}}Y} = \n",
    "\\mathbf{K}_{NM} \\tilde{\\mathbf{P}_{K Y}}.\n",
    "\\end{equation}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{P}_{K Y}}  &= \n",
    "\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{P}_{\\mathbf{\\tilde{\\Phi}} Y} \\\\\n",
    "& = \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\left(\\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}}+ \\lambda \\mathbf{I}_M\\right)^{-1} \n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}  \\mathbf{U}_{MM}^T \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}\\\\\n",
    "& = \n",
    "\\left(\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{1/2}\\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}}\\mathbf{\\Lambda}_{MM}^{1/2}\\mathbf{U}_{MM}^T+ \\lambda \\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}\\mathbf{U}_{MM}^T\\right)^{-1} \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}.\n",
    "\\end{align}\n",
    "\n",
    "Now, by noting that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{1/2} \n",
    "\\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}}\n",
    "\\mathbf{\\Lambda}_{MM}^{1/2}  \\mathbf{U}_{MM}^T  = \n",
    "\\mathbf{K}_{NM}^T \\mathbf{K}_{NM},\n",
    "\\end{equation}\n",
    "\n",
    "we see that the sparse KRR model weights is computed by\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{P}_{K Y}}  = \n",
    "\\left(\\mathbf{K}_{NM}^T \\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1} \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "PKY = Knm_train.T @ Knm_train + regularization * Kmm\n",
    "PKY = np.linalg.pinv(PKY)\n",
    "PKY = PKY @ Knm_train.T @ Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, this trick provides a (in some cases considerable) speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_skrr_train = Knm_train @ PKY\n",
    "Y_skrr_test = Knm_test @ PKY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare our results with those from KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref_krr = KernelRidge(alpha=regularization, **kernel_params)\n",
    "ref_krr.fit(X=K_train, y=Y_train)\n",
    "Y_krr = ref_krr.predict(X=K_test)\n",
    "\n",
    "plot_regression(Y_test[:, 0], Y_krr[:, 0], title=\"KRR\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    Y_skrr_test[:, 0],\n",
    "    title=\"Sparse KRR on {} Environments\".format(n_active),\n",
    "    fig=fig,\n",
    "    ax=axes[1],\n",
    "    **cmaps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "Here our loss function is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_{regr} = \\left\\lVert \\mathbf{Y} - \\mathbf{K}_{NM}\\tilde{\\mathbf{P}_{K Y}}\\right\\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "which we compare with KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict(\n",
    "    [\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            y=Y_test,\n",
    "            yp=Y_krr,\n",
    "        ),\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            y=Y_test,\n",
    "            yp=Y_skrr_test,\n",
    "        ),\n",
    "    ],\n",
    "    headers=[\"KRR\", \"sKRR\"],\n",
    "    title=\"Ridge Regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KernelPCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sparse KernelPCovR, instead of using the Nystr&ouml;m approximation as in previous sparse methods, we formulate sparse KernelPCovR from KernelPCovR in a similar manner to how we derived feature space PCovR from sample space PCovR in the [PCovR Notebook](2_PrincipalCovariatesRegression.ipynb).\n",
    "\n",
    "## A (Very) Quick Recap of Sample and Feature Space PCovR\n",
    "In PCovR, we maximize the similarity\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\tilde{\\mathbf{T}}^T\\mathbf{\\tilde{K}}\\tilde{\\mathbf{T}}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "by taking as our whitened projection $\\tilde{\\mathbf{T}} = \\mathbf{XP}_{X\\tilde{T}}$ the eigenvectors corresponding to the $n_{PCA}$ largest eigenvalues of\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{K}} = \\alpha {\\mathbf{X} \\mathbf{X}^T}\n",
    "    + (1 - \\alpha) {\\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T},\n",
    "\\end{equation}\n",
    "\n",
    "which combines correlations between the samples in feature and property space. \n",
    "\n",
    "If the number of features is less than the number of samples, we can equivalently rewrite our similarity function as\n",
    "\n",
    "\\begin{align}\n",
    "\\rho &= \\operatorname{Tr}\\left(\\mathbf{P}_{X\\tilde{T}}^T\\mathbf{C}^{1/2}\\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{-1/2}\\mathbf{C}^{1/2}\\mathbf{P}_{X\\tilde{T}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "and diagonalize a modified covariance\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{C}} = \\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$ to avoid diagonalizing the $n_{samples} \\times n_{samples}$ matrix $\\tilde{\\mathbf{K}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we just do feature-space PCovR in the RKHS\n",
    "\n",
    "In KernelPCovR, we maximize the similarity\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\tilde{\\mathbf{T}}^T\\mathbf{\\tilde{K}}\\tilde{\\mathbf{T}}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "however here $\\tilde{\\mathbf{T}} = \\mathbf{KP}_{KT}$.\n",
    "We compute the projection in feature space by maximizing:\n",
    "\n",
    "\\begin{align}\n",
    "\\rho = \\operatorname{Tr}\\left(\\mathbf{P}_{K\\tilde{T}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{K\\tilde{T}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{K} = \\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T$.\n",
    "It would make sense to use $\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}$ as our sparse \"kernel\", but we must insert an identity to ensure that its eigenvectors are orthonormal. We use the covariance, defined as $\\mathbf{C} = \\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{\\Phi}}$, giving:\n",
    "\n",
    "\\begin{align}\n",
    "\\rho=\\operatorname{Tr}\\left(\\mathbf{P}_{K\\tilde{T}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2}\n",
    "\\mathbf{C}^{1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{K\\tilde{T}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "which ensures orthonormality, as \n",
    "\n",
    "\\begin{align}\n",
    "\\left(\\mathbf{P}_{K\\tilde{T}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{1/2}\n",
    "\\mathbf{C}^{1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{K\\tilde{T}}\\right)\n",
    "&=\\left(\\mathbf{P}_{K\\tilde{T}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{K\\tilde{T}}\\right)\\\\\n",
    "&=\\left(\\mathbf{P}_{K\\tilde{T}}^T\\mathbf{K}^T\\mathbf{K}\\mathbf{P}_{K\\tilde{T}}\\right)\\\\\n",
    "&=\\left(\\tilde{\\mathbf{T}}^T\\tilde{\\mathbf{T}}\\right)\\\\\n",
    "&=\\mathbf{I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(Phi.T, Phi)\n",
    "\n",
    "v_C, U_C = np.linalg.eigh(C)\n",
    "\n",
    "# U_C/v_C are already sorted, but in *increasing* order, so reverse them\n",
    "U_C = np.flip(U_C, axis=1)\n",
    "v_C = np.flip(v_C, axis=0)\n",
    "\n",
    "U_C = U_C[:, v_C > 0]\n",
    "v_C = v_C[v_C > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In analogy with to feature-space PCovR, we define\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "which evaluates to\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\hat{Y}}\n",
    "\\mathbf{\\hat{Y}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2},\n",
    "\\end{equation}\n",
    "\n",
    "Here we normalize our covariance matrix by the factor $\\operatorname{Tr}(\\mathbf{C})/N$ to achieve a balance between the KRR and KPCA parts.\n",
    "\n",
    "**Note**: Since we use centered $\\mathbf{\\tilde{\\Phi}}$ for the KPCA part, we have to use $\\mathbf{\\tilde{\\Phi}}$ for the KRR part to keep it consistent. We can substitute $\\mathbf{\\hat{Y}}$ with the linear regression solution computed directly in active points RKHS: $\\mathbf{Y}_{sKRR} = \\mathbf{\\tilde{\\Phi}}\\left(\\mathbf{C}+ \\lambda \\mathbf{I}\\right)^{-1}\\mathbf{\\tilde{\\Phi}}^T \\mathbf{Y}$.\n",
    "\n",
    "<!---\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{U}_{MM}^T\n",
    "\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T \n",
    "\\mathbf{Y}\n",
    "\\mathbf{Y}^T\n",
    "\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\n",
    "\\mathbf{U}_{MM}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{U}_{MM}^T\n",
    "\\left(\\mathbf{I}_{M}+ \\lambda \n",
    "\\mathbf{K}_{MM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\\right)^{-1}\n",
    "\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T \n",
    "\\mathbf{Y}\n",
    "\\mathbf{Y}^T\n",
    "\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{I}_{M}+ \\lambda \n",
    "\\mathbf{K}_{MM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\\right)^{-1}\n",
    "\\right)^{-1}\n",
    "\\mathbf{U}_{MM}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "regularization = 1e-6\n",
    "\n",
    "Csqrt = U_C @ np.diagflat(np.sqrt(v_C)) @ U_C.T\n",
    "iCsqrt = U_C @ np.diagflat(1.0 / np.sqrt(v_C)) @ U_C.T\n",
    "\n",
    "C_pca = C / (np.trace(C) / C.shape[0])\n",
    "\n",
    "C_lr = np.linalg.pinv(C + regularization * np.eye(C.shape[0]))\n",
    "C_lr = iCsqrt @ Phi.T @ Phi @ C_lr @ Phi.T @ Y_train.reshape(-1, Y_train.shape[-1])\n",
    "C_lr = C_lr @ C_lr.T\n",
    "\n",
    "Ct = alpha * C_pca + (1 - alpha) * C_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then find the eigendecomposition of \n",
    "$\\mathbf{\\tilde{C}}=\n",
    "\\mathbf{U}_\\mathbf{\\tilde{C}}\\mathbf{\\Lambda}_\\mathbf{\\tilde{C}}\\mathbf{U}_\\mathbf{\\tilde{C}}^T$  and \n",
    "solve for $\\mathbf{P}_{\\tilde{\\Phi} T}$ (again analogous to feature-space PCovR, swapping $\\mathbf{\\tilde{\\Phi}}$ for $\\mathbf{X}$): \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\tilde{\\Phi} T}=\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} \n",
    "\\end{equation}\n",
    "\n",
    "where the $\\hat{\\cdot}$ decoration denotes a truncation to $n_{PCA}$ components, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_Ct, U_Ct = np.linalg.eigh(Ct)\n",
    "\n",
    "# U_Ct/v_Ct are already sorted, but in *increasing* order, so reverse them\n",
    "U_Ct = np.flip(U_Ct, axis=1)\n",
    "v_Ct = np.flip(v_Ct, axis=0)\n",
    "\n",
    "U_Ct = U_Ct[:, v_Ct > 0]\n",
    "v_Ct = v_Ct[v_Ct > 0]\n",
    "\n",
    "PPT = iCsqrt @ U_Ct[:, :n_PC] @ np.diag(np.sqrt(v_Ct[:n_PC]))\n",
    "v_Ct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting into Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our projection in feature space takes the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{\\tilde{\\Phi}}_{NM}\\mathbf{P}_{\\tilde{\\Phi} T}\n",
    "\\end{equation}\n",
    "\n",
    "If we want to project using a kernel rather than a feature space vector, this becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{T} &=  \\left(\\mathbf{K}_{NM}-\\mathbf{\\bar{K}}_M\\right)\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{P}_{\\tilde{\\Phi} T} \\\\\n",
    "&=\\mathbf{K}_{NM}\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} - \\mathbf{\\bar{\\Phi}}\\mathbf{P}_{\\tilde{\\Phi} T} \\\\\n",
    "&= \\mathbf{K}_{NM}\\mathbf{P}_{KT} -  \\mathbf{\\bar{T}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{P}_{K T} = \\mathbf{P}_{K\\Phi} \\mathbf{P}_{\\Phi T} = \n",
    "\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = Umm[:, :n_active-1] @ np.diagflat(1/np.sqrt(vmm[:n_active-1])) @ PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T =  Knm_train @ PKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_skpcovr_test = Knm_test @ PKT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again compare to the non-sparse kernel version, giving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref = KernelPCovR(mixing=alpha, n_components=2, center=True, **kernel_params)\n",
    "ref.fit(X_train, Y_train)\n",
    "t = ref.transform(X_test)\n",
    "y = ref.predict(X_test)\n",
    "\n",
    "plot_projection(\n",
    "    Y_test,\n",
    "    check_mirrors(T_skpcovr_test, t),\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=\"Sparse KernelPCovR\",\n",
    "    **cmaps\n",
    ")\n",
    "plot_projection(Y_test, t, fig=fig, ax=axes[1], title=\"KernelPCovR\", **cmaps)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Properties\n",
    "\n",
    "Property prediction takes the exact same form as in KernelPCovR, except with $\\mathbf{T}$ supplied by our sparse construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTY = T.T @ T\n",
    "PTY = np.linalg.pinv(PTY) @ T.T @ Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = Knm_test @ PKT @ PTY\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    Ypred[:, 0],\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=f\"Sparse KernelPCovR with {n_active} Environments\",\n",
    "    **cmaps,\n",
    ")\n",
    "plot_regression(Y_test[:, 0], y[:, 0], fig=fig, ax=axes[1], title=\"KernelPCovR\", **cmaps)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: CUR Decomposition and Feature Selection\n",
    "\n",
    "Continue on to the [next notebook](5_CUR.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in `skcosmo`\n",
    "\n",
    "Here we have directly implemented sparse methods, many of which do not require the explicit determination of the feature vectors $\\mathbf{\\Phi}$. Instead, it is possible to determine the feature vectors and use simple, linear models for the regression and dimensionality reduction.\n",
    "\n",
    "Currently, `scikit-learn` gives a feature generator under kernel_approximation.Nystroem. This determines a sparse kernel with a random active set, and in `scikit-cosmo` we are actively developing a version where the active set can be predetermined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import Nystroem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.917px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
