{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning lattice parameters of M$_2$AX phases #\n",
    "\n",
    "### MAX phases are a family of laminated materials (https://doi.org/10.1016/j.mattod.2023.11.010), with the generic formula M$_{n+1}$AX$_n$, where M is an early transition metal (e.g. Sc, Ti, V, Cr, Mn, Zr, Nb, Mo, Hf, Ta and others), A is an A-group element (e.g. Al, Si, S, Ga, Ge, Se, Cd, In, Sn and others) and X is B, C or N. These materials have increasingly captivated a lot of attention because of their unique way of combining ceramic and metallic properties into a homogeneous bulk material. Interestingly for both theoretical aspects and practical applications, their versatile physical and chemical properties can be tuned by properly alloying the elements in their different sublattices. Moreover, these materials are of utter importance since they constitute the three-dimensional precursors from which, after exfoliation of the A elements, MXenes 2D materials can be obtained (https://doi.org/10.1021/acs.chemrev.3c00241). ###\n",
    "### ![](Figures/Fig1.png) ###\n",
    "### Figure 1 *(a) Two SEM images of MXenes flakes and (b) a model of a unit cell of a MXene with $n=1$, together with the supercell obtained by replicating the unit cell twice in each direction.* ###\n",
    "### Alloying different elements led to a huge extension of the family of MAX phases, giving rise to both ordered and disordered structures. As a consequence of the very large combinatorial space that can be explored in this way, at least in principle, a lot of new materials are waiting to be synthesized and characterized. In this respect, computational design and modeling plays a crucial role. A fundamental step that precedes experimental attempts of synthesizing complex MAX phases is the computational screening of unstable phases, usually done with the help of density functional theory (DFT) calculations of relative free energies of formation. When dealing with multi-site solid solutions MAX phases, one needs to first perform the full variable-cell relaxation of atomic positions inside a supercell (typically generated via special quasirandom structures, known as SQS, https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.65.353) that models the randomness of the solution. However, estimating initial positions can be quite challenging in these cases, where also Vegard's law starts to fail. ###\n",
    "\n",
    "### In this notebook, you will learn how it is possible to build a machine learning model that predicts lattice parameters of complex M$_2$AX phases with up to five M elements, two A elements, and two X elements, with experimental-level accuracy. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from functions import  permutations, read_data, rescale_output, reduce_features, generate_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After importing everything we need, we can proceed by first collecting the data. The 'input_data_211.csv' file contains the lattice parameters $a$ and $c$ measured experimentally for every M$_2$AX phase synthesized so far (units in Angstrom). The first three columns contain strings that list all the species contained in the M, A and X sublattice, respectively. The next three columns contain instead the relative concentrations of elements in the three sublattices. Let's see how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('input_data_211.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are quite simple 'ternary' (since they only have three elements, one per type) M$_2$AX phases. If you scroll down the file, however, you will find more complex phases with different chemical species in them, for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[[200]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### refers to a M$_2$AX phase that was synthesized alloying five different transition metals, two A elements and carbon. More details can be found in https://doi.org/10.1080/21663831.2021.2017043.\n",
    "\n",
    "### Now we can upload the whole data set using the function read_data, that takes as input the csv file and gives two arrays as output. The X array contains the input features for each structure, and the Y array contains the corresponding lattice parameters $a$ and $c$. The input features are constructed by taking the physical and chemical properties associated with each element entering the MAX phase, together with the relative concentrations. The complete list of fifteen variables used is given in the following table. Data were taken from  https://pubchem.ncbi.nlm.nih.gov/periodic-table/.\n",
    "\n",
    "| Category | Feature |\n",
    "| :-:  | :-:  |\n",
    "| Structural |  Relative concentration <br> Atomic number <br>  Neutron number <br> Atomic mass|\n",
    "| Physical and chemical | Pauling electronegativity  <br>  Ionization energy  <br> Van der Waals radius |\n",
    "| Electronic | Highest $n$ quantum number for $s$ valence shell orbital, Number of electrons in highest $s$ valence shell   <br> Highest $n$ quantum number for $p$ valence shell orbital, Number of electrons in highest $p$ valence shell  <br> Highest $n$ quantum number for $d$ valence shell orbital, Number of electrons in highest $d$ valence shell  <br> Highest $n$ quantum number for $f$ valence shell orbital, Number of electrons in highest $f$ valence shell |\n",
    "\n",
    "### Since up to now, the majority of M$_2$AX phases has been synthesized with at most five M elements, two A elements and two X elements, we concentrated on the generic $(M_{c_{M_1}}^1, M_{c_{M_2}}^2, M_{c_{M_3}}^3, M_{c_{M_4}}^4, M_{c_{M_5}}^5)_2(A_a^1, A_{1-a}^2)(X_x^1, X_{1-x}^2)$ phase. Then, a total of nine possible elements can appear simultaneously in a single phase, and so the total number of features is 9 $\\times$15 =135 features. ###\n",
    "\n",
    "### However, since the list of all elements that are present in the dataset is not extended to the whole periodic table, but it is limited to eleven transition metals, twenty-four A elements and three X elements, a feature reduction is already allowed. For example, the M transition metal elements considered do not have any $p$ orbitals in their valence shell, as X elements do not have any $d$ or $f$ orbitals in their valence shell. Removing these unnecessary variables leads to a total of 117 features. As an example, the electronic configuration of Niobium reads [Kr]5s$^1$4d$^4$, so that we consider 5 as the highest $s$ valence shell quantum number, 1 as the number of electrons in that shell, 4 as the highest $d$ valence shell quantum number and 4 as the electrons in it, 0 for $f$ shells and electrons in them. The generic input vector $x$ for a phase can be written as $x=$(c$_{M_1}$, c$_{M_2}$, ..., c$_{M_5}$, c$_{A_1}$, c$_{A_2}$, c$_{X_1}$, c$_{X_2}$, z$_{M_1}$, z$_{M_2}$, ..., z$_{M_5}$, z$_{A_1}$, z$_{A_2}$, z$_{X_1}$, z$_{X_2}$, ...), where $c_s$ and $z_s$ are concentrations and atomic numbers of $s$ elements, respectively, and so on for the remaining thirteen sets of variables. Only concentrations referring to the same sublattice sum up to one, i.e. $\\sum_i c_{M_i} =1$, $\\sum_i c_{A_i} =1$ and $\\sum_i c_{X_i} =1$. Each input vector can be thought as a horizontal stacking of vectors, each of which has the nine values for the fifteen variables listed above: $x=c+z+...$, where the sum has to be intended as stacking. As a convention, we use 0 for all variables specific to elements that are not present in the generic $(M_{c_{M_1}}^1, M_{c_{M_2}}^2, M_{c_{M_3}}^3, M_{c_{M_4}}^4, M_{c_{M_5}}^5)_2(A_a^1, A_{1-a}^2)(X_x^1, X_{1-x}^2)$ complex M$_2$AX phase. For example, in the system (Ti$_{0.5}$, Zr$_{0.5}$)$_2$AlC, there are only two out of five possible M elements, and one of two A and X elements, so that in its input vector the first nine variables, for concentrations, will be $c=(0.5, 0.5, 0, 0, 0, 1, 0, 1, 0)$. The same applies identically to the other variables. Finally all the features were normalized by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read experimental data\n",
    "\n",
    "X,Y = read_data('input_data_211.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's inspect one of the input vectors together with its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=300)\n",
    "\n",
    "index_of_structure = 42 # change to some number in [0,201] to explore\n",
    "\n",
    "print('Structure #'+str(index_of_structure))\n",
    "data.iloc[[index_of_structure]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1mINPUT\\033[0m for structure #'+str(index_of_structure)+': \\n'+str(X[index_of_structure]))\n",
    "print('\\033[1mOUTPUT\\033[0m for structure #'+str(index_of_structure)+': \\n'+str(Y[index_of_structure]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal is to build a machine learning model that is able to approximate the relationship $f$ between $X$ and $Y$, that is a relationship $f: [0,1]^{117} \\rightarrow \\mathbb{R}^2$. This is called a regression problem.\n",
    "### In general, in the context of machine learning, one defines a training set $ \\tau =\\{(x_i,y_i), x_i \\in X, y_i \\in Y, i=1, ..., n\\} $ by randomly sampling from the original data set and a function $f$ which has a set of parameters, $\\theta$. The aim is then to find the best set of parameters $\\theta^*$ that minimize an \"empirical risk\" function\n",
    "### $\\epsilon_n(f) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i,f_{\\theta}(x_i)) + \\lambda \\Omega(\\theta)$\n",
    "### where $\\ell$ is a loss function that measures the error in predicting $f(x_i)$ instead of $y_i$ and $\\Omega$ is a regularization function for the parameters $\\theta$. The parameter $\\lambda$ is called hyperparameter, since it is not a parameter of the model itself, but rather an external one that is used to minimize the generalization error of the model, i.e. the error of the model on validation data that should mimic the behaviour on unseen data, i.e. the left-over test set. For example, linear regression $f=\\theta^{\\textrm{T}}x$ with square loss function $\\ell = (y_i-\\theta^{\\textrm{T}}x_i)^2$ and Tikhonov (or Ridge) regularization $\\Omega(\\theta) = ||\\theta||^2$ has the well known explicit solution $\\theta^* = (X^{\\textrm{T}}X+\\lambda n I)^{-1}X^{\\textrm{T}}y$, where $X$ is the $n\\times d$ matrix containing the $n$ input training $d$-dimensional vectors, $Y$ is the $n\\times 1$ that contains the relative outputs and $I$ the identity.\n",
    "\n",
    "### Several models have been proposed in the past, with increasing complexity, for solving the most difficult regression tasks, such as support vector machines, neural networks and gradient boosted trees. More information can be found, among a lot of other good references, in the excellent (and free..!) book by Hastie, Tibshirani and Friedman https://hastie.su.domains/ElemStatLearn/ or in the one by Bishop, also free, available at https://www.bishopbook.com/.\n",
    "\n",
    "### In this notebook, we will train an ensemble model, that is a collection of models that share only the same functional form, but that are trained on different train-test splits of the original sets and therefore have different optimal parameters and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose how many models to include (i.e. how many splits of the dataset into training and test set), e.g. N=5 for a quick test, N=30 for a more robust model\n",
    "\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the $N$ models will be trained, the quality of the ensemble model will be determined by some scores, averaged over the $N$ splits. In particular, we will use three different scores:\n",
    "### 1) the $r^2$ score, known as coefficient of determination, defined as $r^2 = 1-\\frac{\\sum_i (y_i-f(x_i))^2}{\\sum_i (y_i-\\bar{y})^2}$, where $\\bar{y}$ is the average of observations $y_i$.\n",
    "### 2) the root mean squared error, defined as $ \\textrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_i(y_i-f(x_i))^2}$\n",
    "### 3) the relative root mean squared error, which is a normalized version of the $\\textrm{RMSE}$, defined as $\\textrm{RRMSE} = \\textrm{RMSE} $ / average of predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vectors to store scores, for both a and c lattice parameters, on both training and test set.\n",
    "\n",
    "a_r2_train = np.empty(0)\n",
    "a_r2_test = np.empty(0)\n",
    "\n",
    "a_rmse_train = np.empty(0)\n",
    "a_rmse_test = np.empty(0)\n",
    "\n",
    "a_rrmse_train = np.empty(0)\n",
    "a_rrmse_test = np.empty(0)\n",
    "\n",
    "c_r2_train = np.empty(0)\n",
    "c_r2_test = np.empty(0)\n",
    "\n",
    "c_rmse_train = np.empty(0)\n",
    "c_rmse_test = np.empty(0)\n",
    "\n",
    "c_rrmse_train = np.empty(0)\n",
    "c_rrmse_test = np.empty(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define which model to use as a regressor $f$ ##\n",
    "### choose from:\n",
    "\n",
    "### LR -> for linear model\n",
    "### TR -> for Tikhonov regression\n",
    "### SVR -> for a Support Vector regressor\n",
    "### RF -> for a Random Forest regressor\n",
    "### LightGBM -> for a Light Gradient Boosting Machines regressor\n",
    "### XGB -> for a Extreme Gradient Boosting regressor\n",
    "\n",
    "### note that different models have different performances (in terms of speed and accuracy) in different data sets. It is therefore advised that you get a feel of what are the training times by making some tests using a small number of models first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = 'LightGBM' \n",
    "\n",
    "save_folder = './MY_LGBM_MODELS_test/'       # create a new directory every time you run another training!\n",
    "\n",
    "if not os.path.isdir(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "# prepare the list and files to store the best models \n",
    "\n",
    "best_models_a = []\n",
    "best_models_a_file = 'best_models_a.sav'  \n",
    "\n",
    "best_models_c = []\n",
    "best_models_c_file = 'best_models_c.sav' \n",
    "\n",
    "\n",
    "   \n",
    "# in the following file we will write the min and max values for both a and c of every split, since we will need them to do the inverse scaling \n",
    "# when using the final ensemble model for predictions\n",
    "\n",
    "min_max_file = open(save_folder+'min_max.txt','w')\n",
    "\n",
    "min_max_file.write('min_a'+' '+'max_a'+' '+'min_c'+' '+'max_c'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "\n",
    "    print('training models on split '+str(i+1)+'/'+str(N))\n",
    "    \n",
    "    # splitting in train and test set\n",
    "\n",
    "    X_tr,X_te,y_tr,y_te = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "    # data augmentation\n",
    "\n",
    "    X_tr, y_tr = permutations(X_tr,y_tr)\n",
    "    X_te, y_te = permutations(X_te,y_te)\n",
    "\n",
    "    # feature reduction\n",
    "\n",
    "    X_tr = reduce_features(X_tr)\n",
    "    X_te = reduce_features(X_te)\n",
    "\n",
    "    # take validation set for hyperparameter optimization\n",
    "\n",
    "    X_trainval, X_val, y_trainval, y_val = train_test_split(X_tr,y_tr, test_size = 0.3)\n",
    "\n",
    "    a_tr = y_tr[:,0]\n",
    "    a_te = y_te[:,0]\n",
    "    a_trainval = y_trainval[:,0]\n",
    "    a_val = y_val[:,0]\n",
    "    \n",
    "    c_tr = y_tr[:,1]\n",
    "    c_te = y_te[:,1]\n",
    "    c_trainval = y_trainval[:,1]\n",
    "    c_val = y_val[:,1]\n",
    "    \n",
    "    # rescale output \n",
    "\n",
    "    max_a = max(a_tr)\n",
    "    min_a = min(a_tr)\n",
    "\n",
    "    max_c = max(c_tr)\n",
    "    min_c = min(c_tr)\n",
    "\n",
    "    # save min and max \n",
    "\n",
    "    min_max_file.write(str(min_a)+' '+str(max_a)+' '+str(min_c)+' '+str(max_c)+'\\n')\n",
    "\n",
    "    a_tr = rescale_output(a_tr, min_a, max_a)\n",
    "    a_te = rescale_output(a_te, min_a, max_a)\n",
    "    a_trainval = rescale_output(a_trainval, min_a, max_a)\n",
    "    a_val = rescale_output(a_val, min_a, max_a)\n",
    "\n",
    "    c_tr = rescale_output(c_tr, min_c, max_c)\n",
    "    c_te = rescale_output(c_te, min_c, max_c)\n",
    "    c_trainval = rescale_output(c_trainval, min_c, max_c)\n",
    "    c_val = rescale_output(c_val, min_c, max_c)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # hyperparameter optimization with optuna\n",
    "    \n",
    "    if model=='LR':\n",
    "    \n",
    "        # in this case there are no hyperparameters, so it is just\n",
    "        \n",
    "        final_reg_a = LinearRegression(n_jobs=-1)\n",
    "        final_reg_c = LinearRegression(n_jobs=-1)\n",
    "        \n",
    "    if model=='TR':\n",
    "        \n",
    "        def objective_a(trial):\n",
    "\n",
    "            alpha = trial.suggest_float('tr_alpha',0.01,1)\n",
    "            tr_model =  KernelRidge(alpha=alpha, kernel='linear')\n",
    "\n",
    "            tr_model.fit(X_trainval, a_trainval)\n",
    "            score = tr_model.score(X_val, a_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        def objective_c(trial):\n",
    "\n",
    "            alpha = trial.suggest_float('tr_alpha',0.01,1)\n",
    "            tr_model =  KernelRidge(alpha=alpha, kernel='linear')\n",
    "            \n",
    "            tr_model.fit(X_trainval, c_trainval)\n",
    "            score = tr_model.score(X_val, c_val)\n",
    "\n",
    "            return score\n",
    "        \n",
    "        study_a = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_a.optimize(objective_a, n_trials = 50)\n",
    "\n",
    "        study_c = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_c.optimize(objective_c, n_trials = 50)\n",
    "\n",
    "        # save best hyperparameter found by maximization of objectives\n",
    "\n",
    "        best_parameters_a = study_a.best_trial.params\n",
    "\n",
    "        best_alpha_a = best_parameters_a.get(\"tr_alpha\")\n",
    "\n",
    "        final_reg_a = KernelRidge(alpha=best_alpha_a, kernel='linear')\n",
    "\n",
    "        best_parameters_c = study_c.best_trial.params\n",
    "\n",
    "        best_alpha_c = best_parameters_c.get(\"tr_alpha\")\n",
    "\n",
    "        final_reg_c = KernelRidge(alpha=best_alpha_c, kernel='linear')\n",
    "    \n",
    "    if model=='SVR':\n",
    "        \n",
    "        def objective_a(trial):\n",
    "\n",
    "            degree = trial.suggest_int('svr_degree',2,10)\n",
    "            epsilon = trial.suggest_float('svr_epsilon',0.001,1)\n",
    "            C = trial.suggest_float('svr_C',1,100)\n",
    "            kernel = trial.suggest_categorical('svr_kernel',[\"linear\", \"poly\", \"rbf\"])\n",
    "\n",
    "            svr_model =  SVR(degree=degree, epsilon=epsilon, C=C, kernel=kernel)\n",
    "\n",
    "            svr_model.fit(X_trainval, a_trainval)\n",
    "            score = svr_model.score(X_val, a_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        def objective_c(trial):\n",
    "\n",
    "            degree = trial.suggest_int('svr_degree',2,10)\n",
    "            epsilon = trial.suggest_float('svr_epsilon',0.001,1)\n",
    "            C = trial.suggest_float('svr_C',1,100)\n",
    "            kernel = trial.suggest_categorical('svr_kernel',[\"linear\", \"poly\", \"rbf\"])\n",
    "\n",
    "            svr_model =  SVR(degree=degree, epsilon=epsilon, C=C, kernel=kernel)\n",
    "\n",
    "            svr_model.fit(X_trainval, c_trainval)\n",
    "            score = svr_model.score(X_val, c_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        study_a = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_a.optimize(objective_a, n_trials = 50, n_jobs=-1)\n",
    "\n",
    "        study_c = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_c.optimize(objective_c, n_trials = 50, n_jobs=-1)\n",
    "\n",
    "        # save best hyperparameter found by maximization of objectives\n",
    "\n",
    "        best_parameters_a = study_a.best_trial.params\n",
    "\n",
    "        best_degree_a = best_parameters_a.get(\"svr_degree\")\n",
    "        best_kernel_a = best_parameters_a.get(\"svr_kernel\")\n",
    "        best_epsilon_a = best_parameters_a.get(\"svr_epsilon\")\n",
    "        best_C_a = best_parameters_a.get(\"svr_C\")\n",
    "\n",
    "        final_reg_a = SVR(degree=best_degree_a, epsilon=best_epsilon_a, C=best_C_a, kernel=best_kernel_a)\n",
    "\n",
    "        best_parameters_c = study_c.best_trial.params\n",
    "\n",
    "        best_degree_c = best_parameters_c.get(\"svr_degree\")\n",
    "        best_kernel_c = best_parameters_c.get(\"svr_kernel\")\n",
    "        best_epsilon_c = best_parameters_c.get(\"svr_epsilon\")\n",
    "        best_C_c = best_parameters_c.get(\"svr_C\")\n",
    "\n",
    "        final_reg_c = SVR(degree=best_degree_c, epsilon=best_epsilon_c, C=best_C_c, kernel=best_kernel_c)\n",
    "\n",
    "    if model=='RF':\n",
    "\n",
    "        def objective_a(trial):\n",
    "\n",
    "            n_estimators = trial.suggest_int('rf_n_est',30,500)\n",
    "            max_depth = trial.suggest_int('rf_max_depth',2,25)\n",
    "            min_samples_split = trial.suggest_int('rf_min_samples_split',2,10)\n",
    "            min_samples_leaf = trial.suggest_int('rf_min_samples_leaf',1,5)\n",
    "            max_samples = trial.suggest_float('rf_sub',0,1)\n",
    "            max_leaf_nodes = trial.suggest_int('rf_n_leaves',3,30)\n",
    "\n",
    "            rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_samples=max_samples, max_leaf_nodes=max_leaf_nodes, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "            rf_model.fit(X_trainval, a_trainval)\n",
    "            score = rf_model.score(X_val, a_val)\n",
    "\n",
    "            return score\n",
    "        \n",
    "        def objective_c(trial):\n",
    "\n",
    "            n_estimators = trial.suggest_int('rf_n_est',30,500)\n",
    "            max_depth = trial.suggest_int('rf_max_depth',2,25)\n",
    "            min_samples_split = trial.suggest_int('rf_min_samples_split',2,10)\n",
    "            min_samples_leaf = trial.suggest_int('rf_min_samples_leaf',1,5)\n",
    "            max_samples = trial.suggest_float('rf_sub',0,1)\n",
    "            max_leaf_nodes = trial.suggest_int('rf_n_leaves',3,30)\n",
    "\n",
    "            rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_samples=max_samples, max_leaf_nodes=max_leaf_nodes, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "            rf_model.fit(X_trainval, c_trainval)\n",
    "            score = rf_model.score(X_val, c_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        study_a = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_a.optimize(objective_a, n_trials = 50)\n",
    "\n",
    "        study_c = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_c.optimize(objective_c, n_trials = 50)\n",
    "\n",
    "        # save best hyperparameter found by maximization of objectives\n",
    "\n",
    "        best_parameters_a = study_a.best_trial.params\n",
    "\n",
    "        best_n_est_a = best_parameters_a.get(\"rf_n_est\")\n",
    "        best_max_depth_a = best_parameters_a.get(\"rf_max_depth\")\n",
    "        best_min_samples_split_a = best_parameters_a.get(\"rf_min_samples_split\")\n",
    "        best_min_samples_leaf_a = best_parameters_a.get(\"rf_min_samples_leaf\")\n",
    "        best_max_samples_a = best_parameters_a.get(\"rf_sub\")\n",
    "        best_max_leaf_nodes_a = best_parameters_a.get(\"rf_n_leaves\")\n",
    "\n",
    "        final_reg_a = RandomForestRegressor(n_estimators=best_n_est_a, max_depth=best_max_depth_a, min_samples_split=best_min_samples_split_a, min_samples_leaf=best_min_samples_leaf_a, max_samples=best_max_samples_a, max_leaf_nodes=best_max_leaf_nodes_a, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "        best_parameters_c = study_c.best_trial.params\n",
    "\n",
    "        best_n_est_c = best_parameters_c.get(\"rf_n_est\")\n",
    "        best_max_depth_c = best_parameters_c.get(\"rf_max_depth\")\n",
    "        best_min_samples_split_c = best_parameters_c.get(\"rf_min_samples_split\")\n",
    "        best_min_samples_leaf_c = best_parameters_c.get(\"rf_min_samples_leaf\")\n",
    "        best_max_samples_c = best_parameters_c.get(\"rf_sub\")\n",
    "        best_max_leaf_nodes_c = best_parameters_c.get(\"rf_n_leaves\")\n",
    "\n",
    "        final_reg_c = RandomForestRegressor(n_estimators=best_n_est_c, max_depth=best_max_depth_c, min_samples_split=best_min_samples_split_c, min_samples_leaf=best_min_samples_leaf_c, max_samples=best_max_samples_c, max_leaf_nodes=best_max_leaf_nodes_c, bootstrap=True, n_jobs=-1)\n",
    "    \n",
    "        \n",
    "    if model=='LightGBM':\n",
    "\n",
    "        def objective_a(trial):\n",
    "\n",
    "            max_depth = trial.suggest_int('lgb_max_depth',2,25)\n",
    "            n_estimators = trial.suggest_int('lgb_n_est',30,500)\n",
    "            num_leaves = trial.suggest_int('lgb_n_leaves',3,30)\n",
    "            learning_rate = trial.suggest_float('lgb_eta',0.01,0.4)\n",
    "            min_split_gain = trial.suggest_float('lgb_min_split_gain',0,1)\n",
    "            subsample = trial.suggest_float('lgb_sub',0,1)\n",
    "            lambd = trial.suggest_float('lgb_lambda',0,10)\n",
    "\n",
    "            lgb_model =  lgb.LGBMRegressor(max_depth=max_depth, n_estimators=n_estimators, num_leaves=num_leaves, learning_rate=learning_rate, min_split_gain=min_split_gain, subsample=subsample, reg_lambda=lambd, verbosity=-1, n_jobs=-1)\n",
    "\n",
    "            lgb_model.fit(X_trainval, a_trainval)\n",
    "            score = lgb_model.score(X_val, a_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        def objective_c(trial):\n",
    "\n",
    "            max_depth = trial.suggest_int('lgb_max_depth',2,25)\n",
    "            n_estimators = trial.suggest_int('lgb_n_est',30,500)\n",
    "            num_leaves = trial.suggest_int('lgb_n_leaves',3,30)\n",
    "            learning_rate = trial.suggest_float('lgb_eta',0.01,0.4)\n",
    "            min_split_gain = trial.suggest_float('lgb_min_split_gain',0,1)\n",
    "            subsample = trial.suggest_float('lgb_sub',0,1)\n",
    "            lambd = trial.suggest_float('lgb_lambda',0,10)\n",
    "\n",
    "            lgb_model =  lgb.LGBMRegressor(max_depth=max_depth, n_estimators=n_estimators, num_leaves=num_leaves, learning_rate=learning_rate, min_split_gain=min_split_gain, subsample=subsample, reg_lambda=lambd, verbosity=-1, n_jobs=-1)\n",
    "\n",
    "            lgb_model.fit(X_trainval, c_trainval)\n",
    "            score = lgb_model.score(X_val, c_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        study_a = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_a.optimize(objective_a, n_trials = 50)\n",
    "\n",
    "        study_c = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_c.optimize(objective_c, n_trials = 50)\n",
    "        # save best hyperparameter found by maximization of objectives\n",
    "\n",
    "        best_parameters_a = study_a.best_trial.params\n",
    "\n",
    "        best_max_depth_a = best_parameters_a.get(\"lgb_max_depth\")\n",
    "        best_n_est_a = best_parameters_a.get(\"lgb_n_est\")\n",
    "        best_n_leaves_a = best_parameters_a.get(\"lgb_n_leaves\")\n",
    "        best_learning_rate_a = best_parameters_a.get(\"lgb_eta\")\n",
    "        best_min_split_gain_a = best_parameters_a.get(\"lgb_min_split_gain\")\n",
    "        best_sub_a = best_parameters_a.get(\"lgb_sub\")\n",
    "        best_lambda_a = best_parameters_a.get(\"lgb_lambda\")\n",
    "\n",
    "        final_reg_a = lgb.LGBMRegressor(max_depth = best_max_depth_a, n_estimators = best_n_est_a, num_leaves = best_n_leaves_a, learning_rate=best_learning_rate_a, min_split_gain=best_min_split_gain_a, subsample = best_sub_a, reg_lambda = best_lambda_a, verbosity = -1, n_jobs=-1)\n",
    "\n",
    "        best_parameters_c = study_c.best_trial.params\n",
    "\n",
    "        best_max_depth_c = best_parameters_c.get(\"lgb_max_depth\")\n",
    "        best_n_est_c = best_parameters_c.get(\"lgb_n_est\")\n",
    "        best_n_leaves_c = best_parameters_c.get(\"lgb_n_leaves\")\n",
    "        best_learning_rate_c = best_parameters_c.get(\"lgb_eta\")\n",
    "        best_min_split_gain_c = best_parameters_c.get(\"lgb_min_split_gain\")\n",
    "        best_sub_c = best_parameters_c.get(\"lgb_sub\")\n",
    "        best_lambda_c = best_parameters_c.get(\"lgb_lambda\")\n",
    "\n",
    "        final_reg_c = lgb.LGBMRegressor(max_depth = best_max_depth_c, n_estimators = best_n_est_c, num_leaves = best_n_leaves_c, learning_rate=best_learning_rate_c, min_split_gain=best_min_split_gain_c, subsample = best_sub_c, reg_lambda = best_lambda_c, verbosity = -1, n_jobs=-1)\n",
    "\n",
    "    elif model=='XGB':\n",
    "\n",
    "        def objective_a(trial):\n",
    "\n",
    "            max_depth = trial.suggest_int('xgb_max_depth',2,25)\n",
    "            n_estimators = trial.suggest_int('xgb_n_est',30,500)\n",
    "            eta = trial.suggest_float('xgb_eta',0.01,0.4)\n",
    "            gamma = trial.suggest_float('xgb_gamma',0,1)\n",
    "            subsample = trial.suggest_float('xgb_sub',0,1)\n",
    "            lambd = trial.suggest_float('xgb_lambda',0,10)\n",
    "\n",
    "            xgb_model = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, eta=eta, gamma=gamma, subsample=subsample, reg_lambda=lambd, n_jobs=-1)\n",
    "\n",
    "            xgb_model.fit(X_trainval, a_trainval)\n",
    "            score = xgb_model.score(X_val, a_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        def objective_c(trial):\n",
    "\n",
    "            max_depth = trial.suggest_int('xgb_max_depth',2,25)\n",
    "            n_estimators = trial.suggest_int('xgb_n_est',30,500)\n",
    "            eta = trial.suggest_float('xgb_eta',0.01,0.4)\n",
    "            gamma = trial.suggest_float('xgb_gamma',0,1)\n",
    "            subsample = trial.suggest_float('xgb_sub',0,1)\n",
    "            lambd = trial.suggest_float('xgb_lambda',0,10)\n",
    "\n",
    "            xgb_model = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, eta=eta, gamma=gamma, subsample=subsample, reg_lambda=lambd, n_jobs=-1)\n",
    "\n",
    "            xgb_model.fit(X_trainval, a_trainval)\n",
    "            score = xgb_model.score(X_val, a_val)\n",
    "\n",
    "            return score\n",
    "\n",
    "        study_a = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_a.optimize(objective_a, n_trials = 50)\n",
    "\n",
    "        study_c = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto', reduction_factor=3))\n",
    "        study_c.optimize(objective_c, n_trials = 50)\n",
    "\n",
    "        # save best hyperparameter found by maximization of objectives\n",
    "\n",
    "        best_parameters_a = study_a.best_trial.params\n",
    "\n",
    "        best_max_depth_a = best_parameters_a.get(\"xgb_max_depth\")\n",
    "        best_n_est_a = best_parameters_a.get(\"xgb_n_est\")\n",
    "        best_eta_a = best_parameters_a.get(\"xgb_eta\")\n",
    "        best_gamma_a = best_parameters_a.get(\"xgb_gamma\")\n",
    "        best_sub_a = best_parameters_a.get(\"xgb_sub\")\n",
    "        best_lambda_a = best_parameters_a.get(\"xgb_lambda\")\n",
    "\n",
    "        final_reg_a = XGBRegressor(max_depth = best_max_depth_a, n_estimators = best_n_est_a, eta=best_eta_a, gamma=best_gamma_a, subsample = best_sub_a, reg_lambda = best_lambda_a, n_jobs=-1)\n",
    "\n",
    "        best_parameters_c = study_c.best_trial.params\n",
    "\n",
    "        best_max_depth_c = best_parameters_c.get(\"xgb_max_depth\")\n",
    "        best_n_est_c = best_parameters_c.get(\"xgb_n_est\")\n",
    "        best_eta_c = best_parameters_c.get(\"xgb_eta\")\n",
    "        best_gamma_c = best_parameters_c.get(\"xgb_gamma\")\n",
    "        best_sub_c = best_parameters_c.get(\"xgb_sub\")\n",
    "        best_lambda_c = best_parameters_c.get(\"xgb_lambda\")\n",
    "\n",
    "        final_reg_c = XGBRegressor(max_depth = best_max_depth_c, n_estimators = best_n_est_c, eta=best_eta_c, gamma=best_gamma_c, subsample = best_sub_c, reg_lambda = best_lambda_c, n_jobs=-1)\n",
    "\n",
    "\n",
    "    # now let's fit two models with the best hyperparameters on the whole training set\n",
    "\n",
    "    final_reg_a.fit(X_tr,a_tr)\n",
    "    final_reg_c.fit(X_tr,c_tr)\n",
    "\n",
    "    # save the scores on training and test set for current split\n",
    "\n",
    "    a_r2_train = np.append(a_r2_train, final_reg_a.score(X_tr,a_tr))\n",
    "    a_r2_test = np.append(a_r2_test, final_reg_a.score(X_te,a_te))\n",
    "\n",
    "    a_tr_pred = final_reg_a.predict(X_tr)\n",
    "    a_te_pred = final_reg_a.predict(X_te)\n",
    "\n",
    "    a_rmse_train = np.append(a_rmse_train, root_mean_squared_error(a_tr, a_tr_pred))\n",
    "    a_rmse_test = np.append(a_rmse_test, root_mean_squared_error(a_te, a_te_pred))\n",
    "\n",
    "    a_rrmse_train = np.append(a_rrmse_train, a_rmse_train/a_tr_pred.mean())\n",
    "    a_rrmse_test = np.append(a_rrmse_test, a_rmse_test/a_te_pred.mean())\n",
    "\n",
    "\n",
    "    c_r2_train = np.append(c_r2_train, final_reg_c.score(X_tr,c_tr))\n",
    "    c_r2_test = np.append(c_r2_test, final_reg_c.score(X_te,c_te))\n",
    "\n",
    "    c_tr_pred = final_reg_c.predict(X_tr)\n",
    "    c_te_pred = final_reg_c.predict(X_te)\n",
    "\n",
    "    c_rmse_train = np.append(c_rmse_train, root_mean_squared_error(c_tr, c_tr_pred))\n",
    "    c_rmse_test = np.append(c_rmse_test, root_mean_squared_error(c_te, c_te_pred))\n",
    "\n",
    "    c_rrmse_train = np.append(c_rrmse_train, c_rmse_train/c_tr_pred.mean())\n",
    "    c_rrmse_test = np.append(c_rrmse_test, c_rmse_test/c_te_pred.mean())\n",
    "\n",
    "    best_models_a.append(final_reg_a)\n",
    "    best_models_c.append(final_reg_c)\n",
    "    \n",
    "min_max_file.close()\n",
    "\n",
    "    \n",
    "pickle.dump(best_models_a, open(save_folder+best_models_a_file, 'wb'))\n",
    "pickle.dump(best_models_c, open(save_folder+best_models_c_file, 'wb'))\n",
    "\n",
    "print('training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To inspect the best models found on each split we have to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models_a = pickle.load(open(save_folder+'best_models_a.sav','rb'))\n",
    "loaded_models_c = pickle.load(open(save_folder+'best_models_c.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so that we can print the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which model to inspect\n",
    "n_model = 0\n",
    "\n",
    "print('Hyperparameters of model '+str(n_model))\n",
    "print(loaded_models_a[n_model].get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average scores and write them in a file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_scores = open(save_folder+'best_results_scores.txt','w')\n",
    "\n",
    "file_scores.write('a r2 train average = '+str(a_r2_train.mean())+' +- '+str(np.std(a_r2_train))+'\\n')\n",
    "file_scores.write('a r2 test average = '+str(a_r2_test.mean())+' +- '+str(np.std(a_r2_test))+'\\n')\n",
    "file_scores.write('\\n')\n",
    "file_scores.write('a rmse train average = '+str(a_rmse_train.mean())+' +- '+str(np.std(a_rmse_train))+'\\n')\n",
    "file_scores.write('a rmse test average = '+str(a_rmse_test.mean())+' +- '+str(np.std(a_rmse_test))+'\\n')\n",
    "file_scores.write('\\n')\n",
    "file_scores.write('a rrmse train average = '+str(a_rrmse_train.mean())+' +- '+str(np.std(a_rrmse_train))+'\\n')\n",
    "file_scores.write('a rrmse test average = '+str(a_rrmse_test.mean())+' +- '+str(np.std(a_rrmse_test))+'\\n')\n",
    "file_scores.write('\\n')\n",
    "file_scores.write('--------------------------------'+'\\n')\n",
    "file_scores.write('\\n')\n",
    "file_scores.write('c r2 train average = '+str(c_r2_train.mean())+' +- '+str(np.std(c_r2_train))+'\\n')\n",
    "file_scores.write('c r2 test average = '+str(c_r2_test.mean())+' +- '+str(np.std(c_r2_test))+'\\n')\n",
    "file_scores.write('\\n')\n",
    "file_scores.write('c rmse train average = '+str(c_rmse_train.mean())+' +- '+str(np.std(c_rmse_train))+'\\n')\n",
    "file_scores.write('c rmse test average = '+str(c_rmse_test.mean())+' +- '+str(np.std(c_rmse_test))+'\\n')\n",
    "file_scores.write('\\n')\n",
    "file_scores.write('c rrmse train average = '+str(c_rrmse_train.mean())+' +- '+str(np.std(c_rrmse_train))+'\\n')\n",
    "file_scores.write('c rrmse test average = '+str(c_rrmse_test.mean())+' +- '+str(np.std(c_rrmse_test))+'\\n')\n",
    "file_scores.write('\\n')\n",
    "\n",
    "file_scores.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the models are saved, let's try to compute the lattice parameters of a hypotethical MAX phase ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the chemical species of the three sublattices and the relative concentrations \n",
    "\n",
    "X_new = generate_features(m_names='Ti,Zr,Nb,Hf,Ta',a_names='Al,Sn',x_names='C',c_m='0.21,0.19,0.18,0.21,0.22',c_a='0.62,0.38',c_x='1')\n",
    "\n",
    "X_new = np.asarray(X_new).reshape(1,-1)\n",
    "\n",
    "# write a fake Y just to call the permutaitons functions\n",
    "\n",
    "Y_new = np.asarray([3.0, 13.0]).reshape(1,-1) \n",
    "\n",
    "X_new_perm, y_new_perm = permutations(X_new,Y_new)\n",
    "\n",
    "X_new_perm = reduce_features(X_new_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the vectors for the predictions of the different models in the ensemble model, on the new generated input vector\n",
    "\n",
    "pred_p_a = np.empty(0)\n",
    "pred_p_c = np.empty(0)\n",
    "pred_i_a = np.empty(0)\n",
    "pred_i_c = np.empty(0)\n",
    "pred_i_err_a = np.empty(0)\n",
    "pred_i_err_c = np.empty(0)\n",
    "\n",
    "# here is where we use the file with min and max values saved above\n",
    "\n",
    "min_max = np.loadtxt(save_folder+'min_max.txt', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the N models\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "\n",
    "    model_a = loaded_models_a[i]\n",
    "    model_c = loaded_models_c[i]\n",
    "\n",
    "    min_a = min_max[i][0]\n",
    "    max_a = min_max[i][1]\n",
    "    min_c = min_max[i][2]\n",
    "    max_c = min_max[i][3]\n",
    "\n",
    "    for x in X_new_perm:\n",
    "\n",
    "        a_new_pred = model_a.predict(x.reshape(1,-1)) # prediction of a and c for each permutation x of the new input \n",
    "        c_new_pred = model_c.predict(x.reshape(1,-1))\n",
    "        \n",
    "        a_pred = (max_a-min_a)*a_new_pred+min_a   # rescale in the original range\n",
    "        c_pred = (max_c-min_c)*c_new_pred+min_c\n",
    "\n",
    "        pred_p_a = np.append(pred_p_a, a_pred)\n",
    "        pred_p_c = np.append(pred_p_c, c_pred)\n",
    "\n",
    "    # save the average predictions on permuations of the i-th model \n",
    "    pred_i_a = np.append(pred_i_a, pred_p_a.mean())\n",
    "    pred_i_c = np.append(pred_i_c, pred_p_c.mean())\n",
    "\n",
    "    # and the same for its errors \n",
    "    pred_i_err_a = np.append(pred_i_err_a, np.std(pred_p_a))\n",
    "    pred_i_err_c = np.append(pred_i_err_c, np.std(pred_p_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have N models for $a$ and N models for $c$. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_a = 1/pred_i_err_a**2\n",
    "weights_c = 1/pred_i_err_c**2\n",
    "\n",
    "a_w_av =  np.dot(weights_a, pred_i_a)/np.sum(weights_a)\n",
    "sigma_a_w_av = 1/math.sqrt(np.sum(weights_a))\n",
    "\n",
    "c_w_av =  np.dot(weights_c, pred_i_c)/np.sum(weights_c)\n",
    "sigma_c_w_av = 1/math.sqrt(np.sum(weights_c))\n",
    "\n",
    "print('average a = '+str(a_w_av)+' +- '+str(sigma_a_w_av))\n",
    "print('average c = '+str(c_w_av)+' +- '+str(sigma_c_w_av))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's visualize the structure ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A VIEW STRUCTURE WITH A SET OF INDICES\n",
    "#\n",
    "from ase.visualize import view\n",
    "import matplotlib.pyplot as plt\n",
    "import nglview as nv\n",
    "def view_structure(structure,myvec=[]):\n",
    "    \"\"\"\n",
    "    Use the ASE library to view an atoms object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    structure: Atoms object\n",
    "    Returns\n",
    "    -------\n",
    "    NGLWidget with GUI: object to be viewed\n",
    "    \n",
    "    \"\"\"\n",
    "    t = nv.ASEStructure(structure)\n",
    "    w = nv.NGLWidget(t, gui=True)\n",
    "    \n",
    "    \n",
    "    from ase.neighborlist import NeighborList\n",
    "    from ase.data import covalent_radii\n",
    "    \n",
    "    # Define cutoff based on typical covalent bond length for Carbon\n",
    "    \n",
    "    cutoff = covalent_radii[6] * 1.2  # Carbon covalent radius * scaling factor\n",
    "    nl = NeighborList([cutoff] * len(structure), skin=0.3, self_interaction=False, bothways=True)\n",
    "    nl.update(structure)\n",
    "    # Print detected bonds\n",
    "    bond_list = []\n",
    "    for i, atom in enumerate(structure):\n",
    "        neighbors = nl.get_neighbors(i)[0]\n",
    "        for n in neighbors:\n",
    "            bond_list.append((i, n))\n",
    "    bonded_atoms = set([index for bond in bond_list for index in bond])\n",
    "    highlight_selection = \" or \".join(map(str, bonded_atoms))\n",
    "    w.add_representation('spacefill', selection=highlight_selection, color=\"red\", radius=0.4)\n",
    "            \n",
    "    w.add_unitcell()\n",
    "\n",
    "    w.add_representation('label',label_type='atomindex',color='black')\n",
    "    w.add_representation('spacefill',selection=myvec,color=\"blue\",radius=0.5)\n",
    "    w.add_representation('licorice', selection=myvec,radius=0.2)  # Adds bonds explicitly\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read, write\n",
    "\n",
    "# we can modify the cell length with the values obtained by the model  predictions:\n",
    "\n",
    "new_a = 4*a_w_av\n",
    "new_b = new_a\n",
    "new_c = c_w_av\n",
    "\n",
    "print(new_a)\n",
    "print(new_b)\n",
    "print(new_c)\n",
    "\n",
    "# use this value in the _cell_length_ parameters in the cif file. Once modified, then use\n",
    "\n",
    "a = read('HE-MAX.cif')\n",
    "\n",
    "view_structure(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MAX phase for which we predicted the lattice parameters values was actually recently synthesized. Experimental values for the lattice parameters are reported to be $a=3.180$ and $c=14.150$. You can test the different base models as well as the number of models in the ensemble models to see how the accuracy in the prediction changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
